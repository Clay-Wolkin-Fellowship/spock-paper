\newcommand{\SAMPN}{100}
\newcommand{\SAMPK}{2000000}
\newcommand{\SAMP}{1400000}
\newcommand{\WARM}{100000}
\newcommand{\COOL}{500000}
\newcommand{\comment}[1]{}

\section{Methodology}
We simulate a set associative cache using a number of different
replacement policies.
We use a statistical sampling technique to keep our total simulation
time down while still ensuring our results are representative of the
overall behavior for the application in question.

Mention PIN, NPB, Custom Python Cache Simulation

\subsection{Sampling Technique}
We sampled the file to minimize the data set we operated on, while maintaining the integrity with results.
	We took \SAMPN~samples of \SAMPK~memory access generated by sp\_omp instrumented by PIN.
	By the central limit theorem we can approximate samples of this size as having a gaussian distribution.
	Each sample was divided into 3 sections, a warm-up period of \WARM~memory accesses,
		a sampling period of \SAMP~memory accesses, and a cool-down period of \COOL~memory accesses.
	For each metric we warmed up the cache during the warm-up period,
		then measured how long it took for any cache line evicted during the sampling period to be recached.
	Cache lines that took longer than the cool-down period to recache were ignored.
	We felt that cache lines that took more than \COOL~memory accesses to recache were unlikely to be interesting.

\subsection{Caching Policies}
\label{sec:policies}

We use a number of different cache replacement policies.  When new data must be added to the cache, either there will already be room in the cache for the data, or something must be evicted from the cache to make room.  The memory index is determined from the memory address of the data.  Each memory index in the cache has one set, and each set has a number of ways, each of which can hold on item of data.  When an item of data must be added to the cache in a set that is full, the replacement policy will determine which determine which data in that set to remove from the cache to make room for the new data.

\subsubsection{Belady}
Andrew should write this section.

\subsubsection{Random}

While Belady's algorithm is the best possible replacement policy, random is perhaps the simplest.  When a set is full, the random replacement policy will delect which way to evict from the cache completely randomly and then place the new data there.

\subsubsection{FIFO}

FIFO, or "first in, first out," is a replacement policy that ignores usage and simply removes the "oldest" data in the set when something must be evicted.  Here, oldest means the data that was added to the cache least recently.

\subsubsection{LRU}

LRU, or "least recently used," is similar to FIFO, but the age of the items is reset when data is used.  This means that even if an item was added least recently, if it was used most recently, then it is treated as the "newest" item in the set.  In this replacement policy, the least recently used item will be removed from the cache, rather than the item originally added the longest time ago.

\subsubsection{MRU}

MRU, or "most recently used," is very similar to LRU, but is the opposite.  Instead of removing the item that was used least recently, this replacement algorithm removes the item that was used most recently.  This means that whatever item was last accessed in the cache will be removed first in the event that something must be evicted.

\subsubsection{NRU FIFO}

NRU, or "not recently used," is slightly more complex.  The NRU replacement algorithm keeps track of whether or not each item in a set has been "recently used."  Every item added to the cache starts off with a value of "not recently used."  Any time an item in the cache is accessed, the value is switched to "recently used."  If all the items in a set are marked as "recently used," then they are all reset to "not recently used."  Any time an eviction must occur, only one of the items marked "not recently used" is evicted.  In the case of NRU FIFO, it uses a FIFO type decision to determine which item to evict.

\subsubsection{NRU Random}

In the case of NRU Random, the cache will evict any one of the items marked "not recently used" at random.

\subsubsection{SRRIP}

SRRIP is like an expansion upon NRU.  Where NRU uses only one bit to track the age of items, SRRIP will use M bits.  Whenever there is a cache hit or its age is set to recently used (0).  New items are given an old age, but not the oldest possible age (for example $2^M - 2$).  For a cache miss, it searches for the first item with the oldest possible age ($2^M - 1$).  If no item has that age, then all the items become one unit older, and then it searches again for one with the oldest possible age.

\subsubsection{BRRIP}

BRRIP is very similar to SRRIP, but in order to avoid cache thrashing, inserts new items with varying age.  Usually, new items will be given the oldest possible age, though with some low probability, new items will be given slightly younger ages.  The rest of the functionality is effectively the same.

\subsubsection{DRRIP}

DRRIP dynamically chooses between SRRIP and BRRIP depending on which one is better at any given time.  Some sets in the cache are permanently set to run on SRRIP and some others are permanently set to run on BRRIP.  DRRIP then keeps track of which policy is doing a better job in general and switches between SRRIP and BRRIP for the rest of the cache accordingly.

